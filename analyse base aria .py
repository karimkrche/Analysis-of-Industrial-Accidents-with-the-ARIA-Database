#!/usr/bin/env python
# coding: utf-8

# In[8]:


import pandas as pd
import json
import sklearn as skl
import matplotlib.pyplot as mtp
import seaborn as sns
import numpy as np
from wordcloud import WordCloud
import spacy
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer


# In[3]:


aria = pd.read_csv('/Users/PC2/Desktop/cours/projet perso/prediction_cause_racines/accidents-tous-req10905-2-.csv',header=5, index_col=[0], quotechar='"', sep=';', encoding='latin1')


# In[3]:


print(type(aria))


# # PR√âPARATION ET NETTOYAGE DES DONN√âES 

# In[4]:


aria.info()


# In[5]:


(aria.isnull().sum())


# In[4]:


#remplissage des NA 
aria.fillna('non communiqu√©', inplace = True)


# In[94]:


aria.head(20)


# ## pretraitement du texte

# In[5]:


import re
from nltk.corpus import stopwords


# On va se concentrer sur les colonnes pertinentes pour la suite de cette analyse 

# In[95]:


#s√©lection des colonnes pertinentes 

data_interet = aria[["Type d'accident", "Type √©v√®nement", "Mati√®res", 
                    "Equipements", "Classe de danger CLP" , "Root causes", 
                    "Disturbances", "Cons√©quences"]].copy()#copie du dataframe original pour √©viter toute confusion


# In[10]:


data_interet.info()


# In[96]:


""""
cr√©er une fonction qui permet de nettoyer les colonnes textuels 
""""
def nettoyage(dataframe):
    for element in dataframe:
        # V√©rifie si la colonne est de type texte avant d'appliquer des transformations
        if dataframe[element].dtype == "object":
            dataframe[element] = dataframe[element].str.lower()  # Convertit en minuscules
            dataframe[element] = dataframe[element].str.strip()  # Supprime les espaces en d√©but et fin
            #dataframe[element] = dataframe[element].str.replace(r'[^\w\s]', '', regex=True)  # Supprime la ponctuation
    print('done !')

nettoyage(data_interet)


# la majorit√© des colonnes poss√®dent plusieurs √©tiquettes par exemple : 
#     Exemple : "Explosion, Incendie" ou "Gaz toxique, Liquide inflammable".
# 
# Ces valeurs ne sont pas directement exploitables pour des analyses statistiques, des corr√©lations, ou des mod√®les pr√©dictifs.
# 
# la Solution Apport√©e ici est la Transformation MultiLabel
# 
# MultiLabelBinarizer transforme chaque √©tiquette pr√©sente dans une colonne en une variable binaire (0 ou 1).
# Chaque nouvelle colonne repr√©sente une √©tiquette unique et indique si elle est pr√©sente (1) ou absente (0) pour une ligne donn√©e.

# In[97]:


from sklearn.preprocessing import MultiLabelBinarizer

def binazer(dataframe, colonnes_interet):
    """
    Applique MultiLabelBinarizer sur les colonnes sp√©cifi√©es dans colonnes_interet.

    Args:
    - dataframe (pd.DataFrame): DataFrame d'entr√©e.
    - colonnes_interet (list): Liste des colonnes √† transformer.

    Returns:
    - pd.DataFrame: DataFrame d'origine avec les colonnes binaris√©es ajout√©es.
    """
    mlb = MultiLabelBinarizer()
    encoded_frames = []  # Liste pour stocker les r√©sultats encod√©s

    for col in colonnes_interet:
        if col in dataframe.columns:
            print(f"Traitement de la colonne : {col}")

            # Transformation des valeurs en listes
            dataframe[col] = dataframe[col].astype(str).str.lower().str.split(',').apply(
                lambda x: [i.strip() for i in x] if isinstance(x, list) else []
            )

            # Appliquer le MultiLabelBinarizer
            etiquettes_encodees = pd.DataFrame(
                mlb.fit_transform(dataframe[col]),
                columns=[f"{col}_{cls}" for cls in mlb.classes_],
                index=dataframe.index
            )
            encoded_frames.append(etiquettes_encodees)

    # Fusionner toutes les colonnes encod√©es
    if encoded_frames:
        encoded_df = pd.concat(encoded_frames, axis=1)
        dataframe = pd.concat([dataframe, encoded_df], axis=1)

    return dataframe


# ### üìà Int√©r√™ts Cl√©s de la Fonction binazer

# #### Structuration des Donn√©es Cat√©gorielles Complexes :

# Transforme des donn√©es multi-√©tiquettes en un format tabulaire clair et interpr√©table.

# #### Compatibilit√© avec les Mod√®les Statistiques et Machine Learning :

# Les algorithmes statistiques et les mod√®les de Machine Learning exigent souvent des variables num√©riques.
# Le format binaire permet d'int√©grer directement ces variables dans des mod√®les.

# #### Am√©lioration de l‚ÄôAnalyse Exploratoire (EDA) :

# Facilite les graphiques et les comparaisons : tu peux d√©sormais compter et comparer la fr√©quence de chaque √©tiquette.
# Permet d'analyser la corr√©lation entre diff√©rentes √©tiquettes.

# #### Cr√©ation de Nouvelles Variables :

# Chaque √©tiquette devient une nouvelle variable ind√©pendante.

# Cela permet une meilleure granularit√© dans l‚Äôanalyse.

# #### Identification des √âtiquettes Dominantes :

# Permet de d√©terminer quelles √©tiquettes sont les plus fr√©quentes et comment elles interagissent avec d'autres variables.

# In[76]:


colonnes_interet = ["Type d'accident", "Type √©v√®nement", "Mati√®res", 
                    "Equipements", "Classe de danger CLP", "Root causes", 
                    "Disturbances", "Cons√©quences"]

data_transformed = binazer(data_interet, colonnes_interet)


# In[45]:


data_transformed.info()


# In[98]:


data_transformed.head(5)


# In[16]:


print(data_transformed.dtypes)


# In[99]:


# Identifier les colonnes contenant des listes ou des types non hashables
non_hashable_cols = [
    col for col in data_transformed.columns
    if data_transformed[col].apply(lambda x: isinstance(x, list)).any()
]

print(f"Colonnes contenant des listes ou des objets non hashables : {non_hashable_cols}")


# In[100]:


for col in non_hashable_cols:
    data_transformed[col] = data_transformed[col].apply(str)


# In[101]:


# V√©rification des doublons

# V√©rifier les doublons
duplicates = data_transformed.duplicated()
print(f"Nombre de lignes dupliqu√©es : {duplicates.sum()}")


# In[102]:


#suppression des doublons 
data_transformed.drop_duplicates
print(f"nombre de ligne apr√®s suppression : {len(data_transformed)}")


# In[103]:


print(data_transformed.info())


# In[79]:


data_num = data_transformed.select_dtypes(include=['number'])


# In[22]:


for col in data_num.columns : 
    print(col)


# In[53]:


frequence = data_num.sum(axis=0)
print(frequence.sort_values(ascending=False))


# # Analyse exploratoire des donn√©es EDA

# ## Observation des √©tiquettes les plus pr√©sentes dans l'analyse 

# In[80]:


def analyse_etiquette (dataframe) : 
    resultats = {}
    for col in dataframe.columns : 
        if "_" in col :
            prefixe = col.split("_")[0]
            if prefixe not in resultats : 
                resultats [prefixe] = []
            resultats[prefixe].append(col)
        
    analyse = {}
    for prefixe, colonnes in resultats.items(): 
        sommes = dataframe[colonnes].sum().sort_values(ascending=False)
        analyse[prefixe] = sommes
        
    return analyse


# In[81]:


analyse_etiket = analyse_etiquette(data_num)


# In[26]:


print(analyse_etiket)


# In[82]:


for cle, valeur in analyse_etiket.items():
    # Trier les fr√©quences de l'√©tiquette en ordre d√©croissant et s√©lectionner les 20 premi√®res
    valeur.sort_values(ascending=False).head(20).plot(kind='bar', figsize=(10, 5))
    
    # Ajouter un titre au graphique
    mtp.title(f"Fr√©quence des 20 √©tiquettes les plus fr√©quentes pour '{cle}'")
    
    # Afficher le graphique
    mtp.show()


# on constate que pour les bariables equipement, root cause, disturbance et mati√®res, beaucoup d'informations sont manquantes 

# In[105]:


#calcul du taux d'informations manquantes 
for col in data_interet.columns :
    taux_nan = (data_interet[col]=="non communiqu√©").mean()*100
    print(f"{col} : {taux_nan :.2f}% des valeurs sont 'non communiqu√©'")


# sans compter le fait que certaines informations sont report√©es comme inconnue

# In[106]:


pourcentages_inconnu = {}

for col in data_interet.columns:
    # Trouver les lignes contenant 'inconnu'
    inconnu = data_interet[col].str.contains(
        r'inconnu[\w]*',  # Regex pour toutes les variantes de 'inconnu'
        case=False,       # Insensible √† la casse
        regex=True,       # Utilise les regex
        na=False          # Ignore les valeurs NaN
    )
    
    # Calculer le pourcentage
    pourcentage_inconnu = inconnu.mean() * 100
    
    # Ajouter le r√©sultat au dictionnaire
    pourcentages_inconnu[col] = pourcentage_inconnu

# Afficher les r√©sultats
for col, pourcentage in pourcentages_inconnu.items():
    print(f"{col} : {pourcentage:.2f}% des valeurs contiennent 'inconnu'")


# En plus nous avons 12% des mati√®res indiqu√©es comme inconnues 

# ### comparaison avant et apr√®s exclusion des NAN 

# In[113]:


variables = ["Equipements", "Root causes", "Mati√®res"]
# Filtrer les lignes o√π la modalit√© n'est pas "Non Communiqu√©e"
data_interet_clean = data_interet.copy()
for col in variables:
    data_interet_clean = data_interet_clean[
        data_interet_clean[col] != "non communiqu√©"
    ]
    
# üõ†Ô∏è √âtape 2 : Distribution des modalit√©s avant exclusion

top_n = 50  # Nombre de modalit√©s √† afficher

for col in variables:
    mtp.figure(figsize=(10, 4))
    data_interet[col].value_counts().head(top_n).plot(
        kind='bar', 
        title=f'Top {top_n} des modalit√©s - {col}'
    )
    mtp.xlabel(col)
    mtp.ylabel('Nombre d\'occurrences')
    mtp.show()
# üõ†Ô∏è √âtape 3 : Distribution des modalit√©s apr√®s exclusion
for col in variables:
    mtp.figure(figsize=(10, 4))
    data_interet_clean[col].value_counts().head(top_n).plot(
        kind='bar', 
        title=f'Top {top_n} des modalit√©s - {col}'
    )
    mtp.xlabel(col)
    mtp.ylabel('Nombre d\'occurrences')
    mtp.show()


# apr√®s exclusion des valeurs inconnues, on constate que les grandes tendances des variables probl√©matiques sont conserv√©es

# ***pour la suite de cette analyse nous allons nous concentrer sur le data sans valeur inconnue pour garantir une analyse plus pr√©cise et fiable. Les valeurs inconnues, lorsqu'elles dominent une variable, peuvent biaiser les distributions, fausser les tests statistiques et perturber les axes factoriels dans une ACM. Cette exclusion permet de concentrer l'analyse sur les relations significatives entre les modalit√©s renseign√©es. N√©anmoins, une comparaison avant/apr√®s exclusion a √©t√© r√©alis√©e pour s'assurer de la robustesse des r√©sultats.*** 
# 
# ***par la suite dans une seconde partie, nous r√©aliserons une autre analyse afin d'identifier les causes racines absentes*** 

# In[114]:


colonnes_interet = ["Type d'accident", "Type √©v√®nement", "Mati√®res", 
                    "Equipements", "Classe de danger CLP", "Root causes", 
                    "Disturbances", "Cons√©quences"]

data_transformed = binazer(data_interet_clean, colonnes_interet)

data_num = data_transformed.select_dtypes(include=['number'])


# # analyse bivari√©e

# In[110]:


#transformer le dico en dataframe 
df_analyse_etiket = pd.DataFrame(analyse_etiket)


# In[111]:


df_analyse_etiket.describe(include="all")


# In[29]:


df_analyse_etiket.info()


# In[115]:


colonnes_avec_prefixe = [col for col in data_num if "_" in col]
print(colonnes_avec_prefixe)


# In[116]:


data_filtre = data_num[colonnes_avec_prefixe]
data_filtre.head(5)


# In[117]:


# 1. Identifier les pr√©fixes des colonnes
prefixes = set(col.split('_')[0] for col in data_filtre.columns if '_' in col)

# 2. Initialiser un DataFrame vide pour stocker les r√©sultats
data_filtre_ts = pd.DataFrame(index=data_filtre.index)

# 3. Boucler sur chaque pr√©fixe et transformer
for prefix in prefixes:
    # Filtrer les colonnes correspondant au pr√©fixe
    columns_with_prefix = [col for col in data_filtre.columns if col.startswith(prefix + '_')]

    # Identifier les modalit√©s associ√©es (colonne avec la valeur 1)
    modalites = data_filtre[columns_with_prefix].idxmax(axis=1).str.split('_').str[1]

    # Ajouter au DataFrame transform√©
    data_filtre_ts[prefix] = modalites


# In[118]:


data_filtre_ts.head(50)


# In[175]:


data_filtre_ts.info()

# Cr√©er des graphiques bivari√©s pour chaque paire de variables qualitatives
def top_modalities(data, col, top_n):
    """
    Filtre les donn√©es pour ne conserver que les top_n modalit√©s les plus fr√©quentes d'une colonne.
    """
    top_values = data[col].value_counts().nlargest(top_n).index
    
    return data[data[col].isin(top_values)]for col1 in data_filtre_ts:
    top_n=20
    for col2 in data_filtre_ts:
        if col1 != col2:
            # Filtrer pour les top_n modalit√©s des deux colonnes
            filtered_data = top_modalities(data_filtre_ts, col1, top_n=20)
            filtered_data = top_modalities(data_filtre_ts, col2, top_n=20)
                
            # Calculer les proportions pour le graphique bivari√©
            proportions = filtered_data.groupby([col1, col2]).size().nlargest(top_n).reset_index(name='Count')
            proportions['Proportion'] = proportions['Count'] / proportions.groupby(col1)['Count'].transform('sum') * 100

            # Cr√©er un graphique en barres empil√©es pour la paire de variables
            mtp.figure(figsize=(10, 5))
            sns.barplot(x=col1, y='Proportion', hue=col2, data=proportions)
            mtp.title(f"Graphique bivari√© entre {col1} et {col2} (proportions)")
            mtp.ylabel('Proportion (%)')
            mtp.xticks(rotation=30)
            mtp.show()
# ## confirmer les observations avec un test du ki 2 

# In[119]:


data_filtre_ts = data_filtre_ts.reset_index(drop=True)
data_filtre_ts.head(5)
#data_filtre_ts_sanstitre = data_filtre_ts.drop(columns=['Titre'])


# In[120]:


#convertir les colonnes en cat√©gorique pour optimiser le temps de calcul 
for col in data_filtre_ts.columns:
    data_filtre_ts[col] = data_filtre_ts[col].astype('category')


# In[121]:


#v√©rifier les modalit√©s des mes colonnes 
for col in data_filtre_ts.columns : 
    print(f'nombre de modalit√© pour {col} :{data_filtre_ts[col].nunique()}')


# In[183]:


#regrouper les modalit√©s les plus rares pour √©quipements et mati√®re 
seuil = 200
for col in ['Equipements','Mati√®res'] : 
    counts = data_filtre_ts[col].value_counts()
    rares = counts[counts<seuil].index
    data_filtre_ts[col] = data_filtre_ts[col].apply(lambda x: 'Autres' if x in rares else x)


# In[184]:


#rev√©rifier les modalit√©s des mes colonnes 
for col in data_filtre_ts.columns : 
    print(f'nombre de modalit√© pour {col} :{data_filtre_ts[col].nunique()}')


# In[185]:


# Reconvertir apr√®s regroupement
for col in data_filtre_ts.columns:
    data_filtre_ts[col] = data_filtre_ts[col].astype('category')


# In[124]:


from scipy.stats import chi2_contingency

# Initialiser les DataFrame pour les coefficients de Cram√©r et les p-values
cramer_v_df = pd.DataFrame(index=data_filtre_ts.columns, columns=data_filtre_ts.columns)
p_value_df = pd.DataFrame(index=data_filtre_ts.columns, columns=data_filtre_ts.columns)
#tschuprow_t_df = pd.DataFrame(index=data_filtre_ts, columns=data_filtre_ts)


# In[125]:


p_value_df.head(5)


# In[126]:



# Calculer le test de chi-deux pour chaque paire de variables qualitatives
for i, column1 in enumerate(data_filtre_ts):
    for j, column2 in enumerate(data_filtre_ts):
        if column1 != column2:
            contingency_table = pd.crosstab(data_filtre_ts[column1], data_filtre_ts[column2])
            chi2, p, dof, expected = chi2_contingency(contingency_table)
            cramer_v = np.sqrt(chi2 / (data_filtre_ts.shape[0] * (min(contingency_table.shape) - 1)))
            #tschuprow_t = cramer_v * np.sqrt((contingency_table.shape[0] - 1) * (contingency_table.shape[1] - 1) / (data_filtre_ts.shape[0] - 1))
            cramer_v_df.loc[column1, column2] = cramer_v
            #tschuprow_t_df.loc[column1, column2] = tschuprow_t
            p_value_df.loc[column1, column2] = p

            
 
# Afficher la DataFrame des p-values
print("\nDataFrame des p-values :")
p_value_df.style.set_properties(**{'border-color': 'black', 'border-width': '1px', 'border-style': 'solid'}) 


# In[127]:


# Afficher la DataFrame des coefficients de Cram√©r
print("DataFrame des coefficients de Cram√©r :")
cramer_v_df.style.background_gradient(cmap='Greens', high=0.4, low=0).set_properties(**{'border-color': 'black','border-width': '1px', 'border-style': 'solid'})


# V de cramer montre des r√©sultats relativement faibles, on constate qu'il y a peu d'association entre les variables. 
# ***La force d'association la plus √©lev√©e est entre le type d'accident et le type d'√©v√®nement 
# concernant les root causes (ce qui nous interresse ici), on observe pas de force d'association √©lev√©e.*** 

# # ACM 

# In[2]:


#importer le package fananalisys
get_ipython().system(' pip install prince')


# In[2]:


import prince


# In[128]:


import fanalysis.mca as mca


# In[5]:


get_ipython().system('pwd')


# In[160]:


X = data_filtre_ts.values


# In[186]:


df=data_filtre_ts
# Cr√©er une instance de la classe MCA
my_mca = mca.MCA(row_labels=df.index.values, var_labels=df.columns)


# In[187]:


my_mca.fit(X)


# In[188]:


print(my_mca.eig_)


# In[189]:


# Graphique des valeurs propres 

my_mca.plot_eigenvalues()


# In[190]:


# Pourcentage de variance expliqu√©
my_mca.plot_eigenvalues(type="percentage")


# In[191]:


# Variance expliqu√©e cumul√©e
my_mca.plot_eigenvalues(type="cumulative")


# 70% du pourcentage cumul√© de variance est expliqu√© par environ 230 axes
# 
# Il y a une grande diversit√© dans les donn√©es, avec beaucoup de variabilit√© r√©partie sur de nombreux axes.
# Les 230 premiers axes principaux contiennent 70% de l'information totale.
# Cela veut dire que :
# L'information est tr√®s dispers√©e sur un grand nombre d'axes.
# Aucune petite poign√©e d'axes (par exemple, les 5 ou 10 premiers) ne suffit √† capturer l'essentiel de l'information.
# Les 230 axes sont n√©cessaires pour garder une vision globale fiable des relations entre variables et individus.
# 
# ***nous allons travailler sur les 180 premiers axes qui expliqueraient 60% d'inertie tout en √©vitant de trop complexifier l'interpr√©tations***

# In[193]:


df=data_filtre_ts
# Cr√©er une instance de la classe MCA
my_mca = mca.MCA(row_labels=df.index.values, var_labels=df.columns, n_components=180)
my_mca.fit(X)


# In[34]:


df_rows = my_mca.row_topandas()
print(df_rows)


# In[35]:


print("Premier Axe")

my_mca.plot_row_cos2(num_axis=1)
print("Deuxi√®me  Axe")

my_mca.plot_row_cos2(num_axis=2)


# In[194]:


print("Premier Axe")

my_mca.plot_col_cos2(num_axis=1)
print("Deuxi√®me  Axe")

my_mca.plot_col_cos2(num_axis=2)
print("Troisi√®me  Axe")

my_mca.plot_col_cos2(num_axis=3)


# In[140]:


print(dir(my_mca))


# In[196]:


my_mca.plot_col_contrib(num_axis=1)
##nb_values=20


# In[173]:


my_mca.plot_col_contrib(num_axis=2, nb_values=20)


# In[174]:


my_mca.plot_col_contrib(num_axis=3, nb_values=20)


# In[197]:


my_mca.mapping_col(num_x_axis=1, num_y_axis=2, figsize=(16, 12))


# Variabilit√© expliqu√©e tr√®s faible :
# Les deux premiers axes ne parviennent pas √† capter une proportion significative de la variance totale.
# Cela signifie qu'il n'y a pas de structure dominante ou de facteur cl√© qui organise les modalit√©s de mani√®re claire.
# 
# R√©partition diffuse des associations :
# Les modalit√©s ne semblent pas se regrouper naturellement autour de quelques axes principaux.
# Il est probable que les relations pertinentes soient dilu√©es sur un nombre √©lev√© d'axes.
# 
# Pistes d'am√©lioration :
# Explorer les axes suppl√©mentaires : Les axes suivants pourraient contenir des informations plus int√©ressantes.
# R√©duire le nombre de modalit√©s : Fusionner ou regrouper les modalit√©s rares sous une cat√©gorie "Autres".
# R√©√©valuer les variables incluses : Certaines variables pourraient ajouter du bruit inutile √† l'analyse.

# In[198]:


my_mca.mapping_col(num_x_axis=3, num_y_axis=4, figsize=(16, 12))


# In[199]:


my_mca.mapping_col(num_x_axis=5, num_y_axis=6, figsize=(16, 12))


# # pr√©diction des causes racines 

# In[5]:


df_na = aria[aria["Root causes"] == 'non communiqu√©'][["Contenu", "Root causes"]].copy()


# In[6]:


df_na.head(10)


# In[7]:


#import de spacy 
import spacy


# In[10]:


#import du param√©trage francais 
nlp = spacy.load("fr_core_news_md")


# In[9]:


"""
cr√©er une fonction qui permet de nettoyer les colonnes textuels en utilisant spacy
"""

def nettoyer_txt_spacy (text) : 
    if pd.isna(text):
        return ''  # G√©rer les valeurs NaN en les rempla√ßant par une cha√Æne vide
    
    # √âtape 1 : Pr√©traitement de base
    text = text.lower().strip()
   
    
    #√©tape 2 tok√©nisation 
    mots_nets = []  # Liste pour stocker les tokens filtr√©s et lemmatis√©s
    doc = nlp(text)
    for token in doc:
        if not token.is_stop and token.is_alpha:
            mots_nets.append(token.lemma_)  # Ajouter le lemme du token
    
    
    # √âtape 3 : Reconstruction du texte
    return ' '.join(mots_nets)


# In[10]:


#appliquer la tokenisation au dataframe 
for col in df_na : 
    if df_na[col].dtype == "object":
        df_na[col] = df_na[col].apply(lambda x: nettoyer_txt_spacy(x))


# In[15]:


df_na.head(5)


# In[26]:


#cr√©er un nuage de mot 

from wordcloud import WordCloud

## wordcloud ne fonctionne pas sur dataframe,
## solution : Concat√©ner les lignes de la colonne en une cha√Æne unique
text = ' '.join(df_na['Contenu'].dropna())


wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
mtp.figure(figsize=(10, 6))
mtp.imshow(wordcloud, interpolation='bilinear')
mtp.axis('off')
mtp.show()


# In[27]:


from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(2, 3), stop_words=["le","la","les","l'","un","une","des","dun"])
X = vectorizer.fit_transform(df_na['Contenu'].dropna())

bigrams = pd.DataFrame(X.sum(axis=0), columns=vectorizer.get_feature_names_out()).T.sort_values(0, ascending=False).head(10)
print(bigrams)


# In[28]:


from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform([text])
print(X.shape)


# In[29]:


#observation des termes avec le score TF le plus √©lev√© 
# R√©cup√©rer les termes et les scores TF-IDF
terms = vectorizer.get_feature_names_out()
score = X.toarray().flatten() #transforme la matrice tf-idf en un tableau num√©rique

# Cr√©er un DataFrame pour visualiser les scores
df_tfidf = pd.DataFrame({'terme' : terms, 'score':score}).sort_values(by='score', ascending=False).head(10)

# Afficher les 10 termes les plus importants
print(df_tfidf)


                  


# # mod√®les pr√©dictifs 

# ***ce qu'on cherche √† pr√©dire est les root cause donc la variable √† pr√©dire Y est la variable root cause***

# In[79]:


#on code la variable √† pr√©dire par une variable binaire 
df_root_cause = aria[["Contenu", "Root causes"]].copy()


# In[6]:


"""
cr√©er une fonction qui permet de nettoyer les colonnes textuels en utilisant spacy
"""

def nettoyer_txt_spacy (text) : 
    if pd.isna(text):
        return ''  # G√©rer les valeurs NaN en les rempla√ßant par une cha√Æne vide
    
    # √âtape 1 : Pr√©traitement de base
    text = text.lower().strip()
   
    
    #√©tape 2 tok√©nisation 
    mots_nets = []  # Liste pour stocker les tokens filtr√©s et lemmatis√©s
    doc = nlp(text)
    for token in doc:
        if not token.is_stop and token.is_alpha:
            mots_nets.append(token.lemma_)  # Ajouter le lemme du token
    
    
    # √âtape 3 : Reconstruction du texte
    return ' '.join(mots_nets)


# In[80]:


for col in df_root_cause : 
    if df_root_cause[col].dtype == "object":
        df_root_cause[col] = df_root_cause[col].apply(lambda x: nettoyer_txt_spacy(x))


# Ici ne pas oublier que nous avons √©norm√©ment de root causes non communiqu√©es, ce qui va fausser les mod√®les d'apprentissages, il est judicieux de s√©parer en deux df les communiqu√©s et non communiqu√©s. les communiqu√©s serviront de train et test

# In[56]:


# S√©parer les donn√©es √©tiquet√©es et non √©tiquet√©es
df_rc = df_root_cause[df_root_cause['Root causes'] != 'non communiquer'].copy()
df_rc_na = df_root_cause[df_root_cause['Root causes'] == 'non communiquer'].copy()

# Afficher les tailles des jeux de donn√©es
print(f"Nombre de lignes pour l'entra√Ænement : {len(df_rc)}")
print(f"Nombre de lignes pour la pr√©diction : {len(df_rc_na)}")


# In[57]:


#identifier les root causes majoritaire 
count_rc = df_rc['Root causes'].value_counts().head(30)
print(count_rc)


# on constate que une classe est surrepr√©sent√©e et qu'elle √©crase toutes les autres 
# Il est judicieux d'appliquer un sous echantillonnage et de regrouper les classes tr√©s minoritaires 

# In[58]:


# D√©finir un seuil de regroupement des classes minoritaires
seuil = 20

# Compter les occurrences des classes dans 'Root causes'
count_rc = df_rc['Root causes'].value_counts()

# Identifier les classes √† regrouper (celles ayant moins de `seuil` occurrences)
classes_a_regrouper = count_rc[count_rc <= seuil].index

# Remplacer ces classes par "Autres"
df_rc['Root causes regroup√©es'] = df_rc['Root causes'].apply(
    lambda x: x if x not in classes_a_regrouper else 'Autres'
)

# V√©rifier la nouvelle r√©partition
print(df_rc['Root causes regroup√©es'].value_counts())


# ### m√©thode par sous-√©chantillonnage 

# In[59]:


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from collections import Counter

#d√©finir X et Y 
X = df_rc['Contenu']
Y = df_rc['Root causes regroup√©es']

"""
Transformation des Root causes en valeurs num√©riques :
pour que les mod√®les supervis√©s puissent traiter les √©tiquettes textuelles.
"""
#encodage de la variable cible 
encode_y = LabelEncoder()
y_encoded = encode_y.fit_transform(Y) 

class_counts = Counter(y_encoded)

# D√©finir un nombre minimum pour chaque classe
sampling_strategy = {cls: min(count, 500) for cls, count in class_counts.items()}

# Appliquer le sous-√©chantillonnage AVANT le split
rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X.to_numpy().reshape(-1, 1), y_encoded)

# Convertir `X_resampled` en s√©rie Pandas pour garder une structure coh√©rente
X_resampled = pd.Series(X_resampled.flatten())


# In[60]:


#observation avant sous echnatillonnage 
mtp.figure(figsize=(12, 6))
df_rc['Root causes regroup√©es'].value_counts().plot(kind='bar', color='skyblue')
mtp.title("üìä R√©partition des Classes AVANT Sous-√âchantillonnage")
mtp.xlabel("Classe")
mtp.ylabel("Nombre d'occurrences")
mtp.xticks(rotation=90)
mtp.show()

#observation apr√®s sous √©chantillonnage 
mtp.figure(figsize=(12, 6))
pd.Series(y_resampled).value_counts().plot(kind='bar', color='orange')
mtp.title("üìä R√©partition des Classes APR√àS Sous-√âchantillonnage")
mtp.xlabel("Classe")
mtp.ylabel("Nombre d'occurrences")
mtp.xticks(rotation=90)
mtp.show()


# In[61]:


#s√©paration 80/20
# S√©paration en 80% train et 20% test
x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# V√©rifier la r√©partition des classes dans train et test
print("R√©partition y_train :", Counter(y_train))
print("R√©partition y_test :", Counter(y_test))


# In[62]:


#s√©paration apprentissage/validation 
from sklearn.feature_extraction.text import TfidfVectorizer


trans_vect = TfidfVectorizer()
"""
Le texte brut est vectoris√© en utilisant TF-IDF, 
ce qui est indispensable pour que les mod√®les puissent interpr√©ter les donn√©es textuelles.
"""

x_train_trans = trans_vect.fit_transform(x_train)
x_test_trans = trans_vect.transform(x_test)


# In[66]:


from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
#on d√©finit deux mod√®eles 
modele_bayes = MultinomialNB(force_alpha=True)  # Evite les erreurs dues aux classes rares
modele_svm = SVC()


# In[67]:


from sklearn.metrics import accuracy_score, classification_report

modele_bayes.fit(x_train_trans,y_train)

# Pr√©dictions
y_pred = modele_bayes.predict(x_test_trans)


# In[68]:


# √âvaluer le mod√®le
print("Pr√©cision :", accuracy_score(y_test, y_pred))


# In[35]:


print(classification_report(y_test, y_pred))


# Les colonnes du rapport contiennent les m√©triques suivantes pour chaque classe‚ÄØ:
# 
# Precision : Proportion des pr√©dictions correctes pour une classe donn√©e parmi toutes les pr√©dictions pour cette classe.
# Recall (Rappel) : Proportion des instances correctement pr√©dites pour une classe donn√©e parmi toutes les instances r√©elles de cette classe.
# F1-score : Moyenne harmonique de la pr√©cision et du rappel (indique l'√©quilibre entre les deux).

# ici on voit clairement que La classe 899 domine le jeu de donn√©es (3526 exemples sur 5449).
# Le mod√®le est tr√®s performant pour cette classe avec un recall (1.00), mais cela peut indiquer un biais en faveur des classes majoritaires.

# D√©s√©quilibre des Classes :
# Une seule classe (899) domine le jeu de donn√©es, ce qui explique pourquoi le mod√®le performe bien pour cette classe mais ignore presque toutes les autres.
# Pr√©cision biais√©e :
# La pr√©cision globale (64,7 %) est biais√©e par la classe majoritaire et ne refl√®te pas la capacit√© du mod√®le √† g√©rer les classes minoritaires.
# Mauvaises performances sur les classes minoritaires :
# Pour la plupart des classes, precision, recall et F1-score sont √† 0.00, ce qui signifie que le mod√®le ne fait pas mieux qu‚Äôun choix al√©atoire pour ces classes.

# In[69]:


#relance avec un nouveau seuil
# D√©finir un seuil de regroupement des classes minoritaires
seuil = 100

# Compter les occurrences des classes dans 'Root causes'
count_rc = df_rc['Root causes'].value_counts()

# Identifier les classes √† regrouper (celles ayant moins de `seuil` occurrences)
classes_a_regrouper = count_rc[count_rc <= seuil].index

# Remplacer ces classes par "Autres"
df_rc['Root causes regroup√©es'] = df_rc['Root causes'].apply(
    lambda x: x if x not in classes_a_regrouper else 'Autres'
)

# V√©rifier la nouvelle r√©partition
print(df_rc['Root causes regroup√©es'].value_counts())

"""
**********************
"""

#d√©finir X et Y 
X = df_rc['Contenu']
Y = df_rc['Root causes regroup√©es']

"""
Transformation des Root causes en valeurs num√©riques :
pour que les mod√®les supervis√©s puissent traiter les √©tiquettes textuelles.
"""
#encodage de la variable cible 
encode_y = LabelEncoder()
y_encoded = encode_y.fit_transform(Y) 

class_counts = Counter(y_encoded)

# D√©finir un nombre minimum pour chaque classe
sampling_strategy = {cls: min(count, 500) for cls, count in class_counts.items()}

# Appliquer le sous-√©chantillonnage AVANT le split
rus = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)
X_resampled, y_resampled = rus.fit_resample(X.to_numpy().reshape(-1, 1), y_encoded)

# Convertir `X_resampled` en s√©rie Pandas pour garder une structure coh√©rente
X_resampled = pd.Series(X_resampled.flatten())


# In[70]:


#observation avant sous echnatillonnage 
mtp.figure(figsize=(12, 6))
df_rc['Root causes regroup√©es'].value_counts().plot(kind='bar', color='skyblue')
mtp.title("üìä R√©partition des Classes AVANT Sous-√âchantillonnage")
mtp.xlabel("Classe")
mtp.ylabel("Nombre d'occurrences")
mtp.xticks(rotation=90)
mtp.show()

#observation apr√®s sous √©chantillonnage 
mtp.figure(figsize=(12, 6))
pd.Series(y_resampled).value_counts().plot(kind='bar', color='orange')
mtp.title("üìä R√©partition des Classes APR√àS Sous-√âchantillonnage")
mtp.xlabel("Classe")
mtp.ylabel("Nombre d'occurrences")
mtp.xticks(rotation=90)
mtp.show()


# In[72]:


#s√©paration 80/20
# S√©paration en 80% train et 20% test
x_train, x_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# V√©rifier la r√©partition des classes dans train et test
print("R√©partition y_train :", Counter(y_train))
print("R√©partition y_test :", Counter(y_test))


# In[73]:


#s√©paration apprentissage/validation 
from sklearn.feature_extraction.text import TfidfVectorizer


trans_vect = TfidfVectorizer()
"""
Le texte brut est vectoris√© en utilisant TF-IDF, 
ce qui est indispensable pour que les mod√®les puissent interpr√©ter les donn√©es textuelles.
"""

x_train_trans = trans_vect.fit_transform(x_train)
x_test_trans = trans_vect.transform(x_test)

#relance du mod√®le bayes
modele_bayes.fit(x_train_trans,y_train)

y_pred = modele_bayes.predict(x_test_trans)

# √âvaluer le mod√®le
print("Pr√©cision :", accuracy_score(y_test, y_pred))


# on va tester avec ***complementNB*** 
# mais la diff√©rence entre les classes sont trop extr√™me 
# on va donc continuer √† sous √©chantillonner mais uniquement la classe majoritaire 

# In[76]:


#sous √©chantillonnage sur la classe majoritaire 
from imblearn.under_sampling import RandomUnderSampler

# Trouver la classe majoritaire
class_counts = Counter(y_encoded)
major_class = max(class_counts, key=class_counts.get)
under_sampler = RandomUnderSampler(sampling_strategy={major_class: 5000}, random_state=42)

# Appliquer le sous-√©chantillonnage
X_resampled, y_resampled = under_sampler.fit_resample(X.to_numpy().reshape(-1, 1), y_encoded)

print("Nouvelle distribution des classes :", Counter(y_resampled))


# In[77]:


from sklearn.naive_bayes import ComplementNB


# Initialisation du mod√®le
complement_nb = ComplementNB()

# Entra√Ænement sur les donn√©es
complement_nb.fit(x_train_trans, y_train)

# Pr√©dictions
y_pred = complement_nb.predict(x_test_trans)

# √âvaluation du mod√®le
print("Accuracy :", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))


# In[83]:


#relance avec un nouveau seuil
# D√©finir un seuil de regroupement des classes minoritaires
seuil = 100

# Compter les occurrences des classes dans 'Root causes'
count_rc = df_rc['Root causes'].value_counts()

# Identifier les classes √† regrouper (celles ayant moins de `seuil` occurrences)
classes_a_regrouper = count_rc[count_rc <= seuil].index

# Remplacer ces classes par "Autres"
df_rc['Root causes regroup√©es'] = df_rc['Root causes'].apply(
    lambda x: x if x not in classes_a_regrouper else 'Autres'
)

# V√©rifier la nouvelle r√©partition
print(df_rc['Root causes regroup√©es'].value_counts())

"""
**********************
"""

#d√©finir X et Y 
X = df_rc['Contenu']
Y = df_rc['Root causes regroup√©es']

"""
Transformation des Root causes en valeurs num√©riques :
pour que les mod√®les supervis√©s puissent traiter les √©tiquettes textuelles.
"""
#encodage de la variable cible 
encode_y = LabelEncoder()
y_encoded = encode_y.fit_transform(Y) 

class_counts = Counter(y_encoded)

# Identifier la classe majoritaire et les classes minoritaires
major_class = max(class_counts, key=class_counts.get)  # Classe la plus fr√©quente
print(f"Classe majoritaire identifi√©e : {major_class}, Nombre d'occurrences : {class_counts[major_class]}")


# En sous √©chantillonnant la classe majoritaire on risque de perdre trop d'information, √† c√¥t√© de √ßa les autres classes sont faibles... ***Pour compenser on va sur √©chantillonner les autres classes***

# In[88]:


from imblearn.over_sampling import SMOTE

# ‚úÖ √âtape 1 : Vectorisation du texte
vectorizer = TfidfVectorizer(max_features=5000)  # On limite √† 5000 features pour √©viter la surcharge
X_vectorized = vectorizer.fit_transform(X).toarray()  # Conversion en array pour compatibilit√©

# ‚úÖ √âtape 2 : Sous-√©chantillonnage de la classe majoritaire
under_sampler = RandomUnderSampler(sampling_strategy={major_class: 5000}, random_state=42)
X_under, y_under = under_sampler.fit_resample(X_vectorized, y_encoded)

print("R√©partition apr√®s sous-√©chantillonnage :", Counter(y_under))

# ‚úÖ √âtape 3 : Sur-√©chantillonnage des classes minoritaires (moins de 500 occurrences)
minor_classes = {cls: 500 for cls, count in Counter(y_under).items() if count < 500}
smote = SMOTE(sampling_strategy=minor_classes, random_state=42)

# **Important** : SMOTE ne fonctionne qu'avec des **donn√©es num√©riques**, donc on utilise `X_under`
X_resampled, y_resampled = smote.fit_resample(X_under, y_under)

print("R√©partition apr√®s sur-√©chantillonnage :", Counter(y_resampled))

# ‚úÖ √âtape 4 : **Split final**
x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

print("R√©partition finale dans y_train :", Counter(y_train_final))
print("R√©partition finale dans y_test :", Counter(y_test_final))


# In[89]:


# Initialisation du mod√®le
complement_nb = ComplementNB()

# Entra√Ænement sur les donn√©es
complement_nb.fit(x_train_final, y_train_final)

# Pr√©dictions
y_pred = complement_nb.predict(x_test_final)

# √âvaluation du mod√®le
print("Accuracy :", accuracy_score(y_test_final, y_pred))
print(classification_report(y_test_final, y_pred))


# In[91]:


classe_predite = encode_y.inverse_transform(y_encoded)
print(classe_predite)


# In[92]:


#avec un seuil moins √©lev√©
seuil = 50

# Compter les occurrences des classes dans 'Root causes'
count_rc = df_rc['Root causes'].value_counts()

# Identifier les classes √† regrouper (celles ayant moins de `seuil` occurrences)
classes_a_regrouper = count_rc[count_rc <= seuil].index

# Remplacer ces classes par "Autres"
df_rc['Root causes regroup√©es'] = df_rc['Root causes'].apply(
    lambda x: x if x not in classes_a_regrouper else 'Autres'
)

# V√©rifier la nouvelle r√©partition
print(df_rc['Root causes regroup√©es'].value_counts())

"""
**********************
"""

#d√©finir X et Y 
X = df_rc['Contenu']
Y = df_rc['Root causes regroup√©es']

"""
Transformation des Root causes en valeurs num√©riques :
pour que les mod√®les supervis√©s puissent traiter les √©tiquettes textuelles.
"""
#encodage de la variable cible 
encode_y = LabelEncoder()
y_encoded = encode_y.fit_transform(Y) 

class_counts = Counter(y_encoded)

# Identifier la classe majoritaire et les classes minoritaires
major_class = max(class_counts, key=class_counts.get)  # Classe la plus fr√©quente
print(f"Classe majoritaire identifi√©e : {major_class}, Nombre d'occurrences : {class_counts[major_class]}")


# In[93]:


# ‚úÖ √âtape 1 : Vectorisation du texte
vectorizer = TfidfVectorizer(max_features=5000)  # On limite √† 5000 features pour √©viter la surcharge
X_vectorized = vectorizer.fit_transform(X).toarray()  # Conversion en array pour compatibilit√©

# ‚úÖ √âtape 2 : Sous-√©chantillonnage de la classe majoritaire
under_sampler = RandomUnderSampler(sampling_strategy={major_class: 5000}, random_state=42)
X_under, y_under = under_sampler.fit_resample(X_vectorized, y_encoded)

print("R√©partition apr√®s sous-√©chantillonnage :", Counter(y_under))

# ‚úÖ √âtape 3 : Sur-√©chantillonnage des classes minoritaires (moins de 500 occurrences)
minor_classes = {cls: 500 for cls, count in Counter(y_under).items() if count < 500}
smote = SMOTE(sampling_strategy=minor_classes, random_state=42)

# **Important** : SMOTE ne fonctionne qu'avec des **donn√©es num√©riques**, donc on utilise `X_under`
X_resampled, y_resampled = smote.fit_resample(X_under, y_under)

print("R√©partition apr√®s sur-√©chantillonnage :", Counter(y_resampled))

# ‚úÖ √âtape 4 : **Split final**
x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

print("R√©partition finale dans y_train :", Counter(y_train_final))
print("R√©partition finale dans y_test :", Counter(y_test_final))


# In[94]:


# Initialisation du mod√®le
complement_nb = ComplementNB()

# Entra√Ænement sur les donn√©es
complement_nb.fit(x_train_final, y_train_final)

# Pr√©dictions
y_pred = complement_nb.predict(x_test_final)

# √âvaluation du mod√®le
print("Accuracy :", accuracy_score(y_test_final, y_pred))
print(classification_report(y_test_final, y_pred))


# In[95]:


#avec un seuil moins √©lev√©
seuil = 20

# Compter les occurrences des classes dans 'Root causes'
count_rc = df_rc['Root causes'].value_counts()

# Identifier les classes √† regrouper (celles ayant moins de `seuil` occurrences)
classes_a_regrouper = count_rc[count_rc <= seuil].index

# Remplacer ces classes par "Autres"
df_rc['Root causes regroup√©es'] = df_rc['Root causes'].apply(
    lambda x: x if x not in classes_a_regrouper else 'Autres'
)

# V√©rifier la nouvelle r√©partition
print(df_rc['Root causes regroup√©es'].value_counts())

"""
**********************
"""

#d√©finir X et Y 
X = df_rc['Contenu']
Y = df_rc['Root causes regroup√©es']


#encodage de la variable cible 
encode_y = LabelEncoder()
y_encoded = encode_y.fit_transform(Y) 

class_counts = Counter(y_encoded)

# Identifier la classe majoritaire et les classes minoritaires
major_class = max(class_counts, key=class_counts.get)  # Classe la plus fr√©quente
print(f"Classe majoritaire identifi√©e : {major_class}, Nombre d'occurrences : {class_counts[major_class]}")


# ‚úÖ √âtape 1 : Vectorisation du texte
vectorizer = TfidfVectorizer(max_features=5000)  # On limite √† 5000 features pour √©viter la surcharge
X_vectorized = vectorizer.fit_transform(X).toarray()  # Conversion en array pour compatibilit√©

# ‚úÖ √âtape 2 : Sous-√©chantillonnage de la classe majoritaire
under_sampler = RandomUnderSampler(sampling_strategy={major_class: 5000}, random_state=42)
X_under, y_under = under_sampler.fit_resample(X_vectorized, y_encoded)

print("R√©partition apr√®s sous-√©chantillonnage :", Counter(y_under))

# ‚úÖ √âtape 3 : Sur-√©chantillonnage des classes minoritaires (moins de 500 occurrences)
minor_classes = {cls: 500 for cls, count in Counter(y_under).items() if count < 500}
smote = SMOTE(sampling_strategy=minor_classes, random_state=42)

# **Important** : SMOTE ne fonctionne qu'avec des **donn√©es num√©riques**, donc on utilise `X_under`
X_resampled, y_resampled = smote.fit_resample(X_under, y_under)

print("R√©partition apr√®s sur-√©chantillonnage :", Counter(y_resampled))

# ‚úÖ √âtape 4 : **Split final**
x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(
    X_resampled, y_resampled, test_size=0.2, random_state=42
)

"""************lancement du mod√®le**********************
"""


# Initialisation du mod√®le
complement_nb = ComplementNB()

# Entra√Ænement sur les donn√©es
complement_nb.fit(x_train_final, y_train_final)

# Pr√©dictions
y_pred = complement_nb.predict(x_test_final)

# √âvaluation du mod√®le
print("Accuracy :", accuracy_score(y_test_final, y_pred))
print(classification_report(y_test_final, y_pred))


# ### Pr√©diction des root causes avec ComplementNB

# In[96]:


#pr√©diction du mod√®le complemntNB 

#d√©finir X 
X_na = df_rc_na['Contenu']

#transformer le texte avec le m√™me vectorizer entra√Æn√©
X_na_transformed = vectorizer.transform(X_na)

# Pr√©dire les Root Causes
y_na_pred = complement_nb.predict(X_na_transformed)

# Convertir les labels pr√©dits en cat√©gories textuelles
y_na_pred_text = encode_y.inverse_transform(y_na_pred)

# Ajouter les pr√©dictions dans le dataframe
df_rc_na['Root causes pr√©dites'] = y_na_pred_text


# In[98]:


#visualisation des root causes
root_causes_counts = df_rc_na['Root causes pr√©dites'].value_counts()

mtp.figure(figsize=(12, 6))

mtp.bar(root_causes_counts.index, root_causes_counts.values, color='skyblue')
mtp.xlabel("Root Causes Pr√©dites")
mtp.ylabel("Nombre d'occurrences")
mtp.title("Distribution des Root Causes Pr√©dites")
mtp.xticks(rotation=90)  # Faire pivoter les √©tiquettes si elles sont longues

mtp.show()


# In[99]:


#g√©n√©rer un nuage de point sur le dataframe des root causes non identifi√©es 
text = ' '.join(df_rc_na['Contenu'].dropna())


wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
mtp.figure(figsize=(10, 6))
mtp.imshow(wordcloud, interpolation='bilinear')
mtp.axis('off')
mtp.show()


# In[ ]:




